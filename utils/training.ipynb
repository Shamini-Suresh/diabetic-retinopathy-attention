{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deba65-adcc-4738-8a24-332616d7e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training utilities for k-fold cross-validation\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "\n",
    "\n",
    "def train_kfold(model_class, backbone, device, train_dataset, k_folds=5, epochs=25, \n",
    "                batch_size=16, use_attention=False, attention_reduction=16):\n",
    "    \"\"\"\n",
    "    Train model with k-fold cross-validation\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class (APTOSVanillaNet or APTOSAttentionNet)\n",
    "        backbone: Backbone architecture name\n",
    "        device: torch device\n",
    "        train_dataset: Training dataset\n",
    "        k_folds: Number of folds for CV\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        use_attention: Whether to use attention (for APTOSAttentionNet)\n",
    "        attention_reduction: Attention reduction ratio\n",
    "    \n",
    "    Returns:\n",
    "        best_model, fold_histories, fold_results\n",
    "    \"\"\"\n",
    "    # Get labels for stratified split\n",
    "    labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "    skfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "    # Storage for results\n",
    "    fold_histories = []\n",
    "    fold_results = []\n",
    "    best_val_kappa = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(skfold.split(np.zeros(len(labels)), labels)):\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'FOLD {fold+1}/{k_folds}')\n",
    "        print(f'Training samples: {len(train_ids)}, Validation samples: {len(val_ids)}')\n",
    "        print(f'{\"=\"*60}')\n",
    "\n",
    "        # Prepare data loaders\n",
    "        train_labels = [labels[i] for i in train_ids]\n",
    "        train_weights = [class_weights[label] for label in train_labels]\n",
    "        train_sampler = WeightedRandomSampler(train_weights, len(train_weights))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            Subset(train_dataset, train_ids),\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=device.type == 'cuda',\n",
    "            persistent_workers=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            Subset(train_dataset, val_ids),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=device.type == 'cuda',\n",
    "            persistent_workers=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        # Create model\n",
    "        if use_attention:\n",
    "            model = model_class(\n",
    "                backbone=backbone,\n",
    "                dropout_rate=0.4,\n",
    "                num_classes=5,\n",
    "                use_attention=True,\n",
    "                attention_reduction=attention_reduction\n",
    "            ).to(device)\n",
    "        else:\n",
    "            model = model_class(\n",
    "                backbone=backbone,\n",
    "                dropout_rate=0.4,\n",
    "                num_classes=5\n",
    "            ).to(device)\n",
    "\n",
    "        # Loss, optimizer, scheduler\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "        # Early stopping\n",
    "        patience, counter = 15, 0\n",
    "        fold_best_kappa = 0\n",
    "\n",
    "        # Fold history\n",
    "        fold_history = {\n",
    "            'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
    "            'val_kappa': [], 'val_weighted_kappa': [], 'val_f1_macro': [], 'val_f1_weighted': []\n",
    "        }\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Train phase\n",
    "            model.train()\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "            for batch_idx, (images, labels_batch, _) in enumerate(tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{epochs}\")):\n",
    "                images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "\n",
    "                if torch.isnan(images).any() or torch.isinf(images).any():\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if scaler:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                        if torch.isnan(loss) or torch.isinf(loss):\n",
    "                            continue\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                        continue\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                    optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_correct += (outputs.argmax(1) == labels_batch).sum().item()\n",
    "                train_total += labels_batch.size(0)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total, val_preds, val_labels = 0, 0, 0, [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels_batch, _ in tqdm(val_loader, desc=\"Validating\"):\n",
    "                    images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "\n",
    "                    if torch.isnan(images).any() or torch.isinf(images).any():\n",
    "                        continue\n",
    "\n",
    "                    with torch.amp.autocast(device_type='cuda') if scaler else contextlib.nullcontext():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                        if torch.isnan(loss) or torch.isinf(loss):\n",
    "                            continue\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (outputs.argmax(1) == labels_batch).sum().item()\n",
    "                    val_total += labels_batch.size(0)\n",
    "                    val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                    val_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            avg_train_loss = train_loss / max(len(train_loader), 1)\n",
    "            avg_val_loss = val_loss / max(len(val_loader), 1)\n",
    "            train_accuracy = train_correct / max(train_total, 1)\n",
    "            val_accuracy = val_correct / max(val_total, 1)\n",
    "\n",
    "            if len(val_preds) > 0 and len(val_labels) > 0:\n",
    "                val_kappa = cohen_kappa_score(val_labels, val_preds, weights='quadratic')\n",
    "                val_weighted_kappa = cohen_kappa_score(val_labels, val_preds, weights='linear')\n",
    "                val_f1_macro = f1_score(val_labels, val_preds, average='macro', zero_division=0)\n",
    "                val_f1_weighted = f1_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
    "            else:\n",
    "                val_kappa = val_weighted_kappa = val_f1_macro = val_f1_weighted = 0.0\n",
    "\n",
    "            # Store metrics\n",
    "            fold_history['train_loss'].append(avg_train_loss)\n",
    "            fold_history['train_acc'].append(train_accuracy)\n",
    "            fold_history['val_loss'].append(avg_val_loss)\n",
    "            fold_history['val_acc'].append(val_accuracy)\n",
    "            fold_history['val_kappa'].append(val_kappa)\n",
    "            fold_history['val_weighted_kappa'].append(val_weighted_kappa)\n",
    "            fold_history['val_f1_macro'].append(val_f1_macro)\n",
    "            fold_history['val_f1_weighted'].append(val_f1_weighted)\n",
    "\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "            print(f'  Val Kappa: {val_kappa:.4f}, F1 Macro: {val_f1_macro:.4f}')\n",
    "\n",
    "            # Update best\n",
    "            if val_kappa > fold_best_kappa:\n",
    "                fold_best_kappa = val_kappa\n",
    "\n",
    "            if val_kappa > best_val_kappa and not (avg_val_loss > avg_train_loss * 3.0) and not np.isnan(val_kappa):\n",
    "                best_val_kappa = val_kappa\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                counter = 0\n",
    "                print(f'‚úì New best validation kappa: {val_kappa:.4f}')\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        # Store fold results\n",
    "        fold_histories.append(fold_history)\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'best_val_kappa': fold_best_kappa,\n",
    "            'best_val_acc': max(fold_history['val_acc']) if fold_history['val_acc'] else 0,\n",
    "            'best_val_f1_macro': max(fold_history['val_f1_macro']) if fold_history['val_f1_macro'] else 0,\n",
    "            'history': fold_history\n",
    "        })\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Create best model\n",
    "    if use_attention:\n",
    "        best_model = model_class(\n",
    "            backbone=backbone,\n",
    "            dropout_rate=0.4,\n",
    "            num_classes=5,\n",
    "            use_attention=True,\n",
    "            attention_reduction=attention_reduction\n",
    "        ).to(device)\n",
    "    else:\n",
    "        best_model = model_class(\n",
    "            backbone=backbone,\n",
    "            dropout_rate=0.4,\n",
    "            num_classes=5\n",
    "        ).to(device)\n",
    "\n",
    "    if best_model_state:\n",
    "        best_model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\nüèÜ Best validation kappa: {best_val_kappa:.4f}\")\n",
    "\n",
    "    return best_model, fold_histories, fold_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ma_detection_env)",
   "language": "python",
   "name": "ma_detection_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
