{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51902b-9bf1-421f-9e5b-66e97433698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization utilities for training history, metrics, and attention maps\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def plot_training_history(fold_histories, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot mean ± std training history across all folds\n",
    "    \n",
    "    Args:\n",
    "        fold_histories: List of fold history dictionaries\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    metrics = ['train_loss', 'val_loss', 'train_acc', 'val_acc',\n",
    "               'val_kappa', 'val_weighted_kappa', 'val_f1_macro', 'val_f1_weighted']\n",
    "    titles = ['Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy',\n",
    "              'Validation Kappa (Quadratic)', 'Validation Kappa (Linear)', \n",
    "              'F1 Score (Macro)', 'F1 Score (Weighted)']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('Training History Across All Folds', fontsize=16, fontweight='bold')\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        if fold_histories:\n",
    "            max_epochs = max(len(fold_history.get(metric, [])) for fold_history in fold_histories)\n",
    "\n",
    "            epoch_values = []\n",
    "            for epoch in range(max_epochs):\n",
    "                values_at_epoch = []\n",
    "                for fold_history in fold_histories:\n",
    "                    if metric in fold_history and epoch < len(fold_history[metric]):\n",
    "                        values_at_epoch.append(fold_history[metric][epoch])\n",
    "                epoch_values.append(values_at_epoch)\n",
    "\n",
    "            means, stds, valid_epochs = [], [], []\n",
    "\n",
    "            for epoch, values in enumerate(epoch_values):\n",
    "                if len(values) > 0:\n",
    "                    means.append(np.mean(values))\n",
    "                    stds.append(np.std(values))\n",
    "                    valid_epochs.append(epoch + 1)\n",
    "\n",
    "            if means:\n",
    "                means = np.array(means)\n",
    "                stds = np.array(stds)\n",
    "\n",
    "                ax.plot(valid_epochs, means, 'b-', linewidth=3, label='Mean', alpha=0.9)\n",
    "                ax.fill_between(valid_epochs, means - stds, means + stds,\n",
    "                               alpha=0.3, color='blue', label='± 1 Std')\n",
    "\n",
    "                if 'loss' in metric:\n",
    "                    best_idx = np.argmin(means)\n",
    "                    best_value = means[best_idx]\n",
    "                else:\n",
    "                    best_idx = np.argmax(means)\n",
    "                    best_value = means[best_idx]\n",
    "\n",
    "                ax.plot(valid_epochs[best_idx], best_value, 'ro', markersize=10,\n",
    "                       label=f'Best: {best_value:.4f}')\n",
    "\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Training history saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_per_class_metrics(results, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot detailed per-class metrics visualization\n",
    "    \n",
    "    Args:\n",
    "        results: Evaluation results dictionary\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Per-Class Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # F1 scores\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.bar(class_names, results['per_class_f1'], alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Per-Class F1 Scores', fontweight='bold')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, value in zip(bars1, results['per_class_f1']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # AUC scores\n",
    "    ax2 = axes[0, 1]\n",
    "    valid_auc = [auc if not np.isnan(auc) else 0 for auc in results['per_class_auc']]\n",
    "    bars2 = ax2.bar(class_names, valid_auc, alpha=0.7, color='lightcoral')\n",
    "    ax2.set_title('Per-Class AUC Scores', fontweight='bold')\n",
    "    ax2.set_ylabel('AUC Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, value in zip(bars2, valid_auc):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Precision vs Recall\n",
    "    ax3 = axes[1, 0]\n",
    "    x_pos = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "\n",
    "    bars3a = ax3.bar(x_pos - width/2, results['per_class_precision'], width,\n",
    "                     label='Precision', alpha=0.7, color='gold')\n",
    "    bars3b = ax3.bar(x_pos + width/2, results['per_class_recall'], width,\n",
    "                     label='Recall', alpha=0.7, color='lightgreen')\n",
    "\n",
    "    ax3.set_title('Per-Class Precision vs Recall', fontweight='bold')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(class_names, rotation=45)\n",
    "    ax3.legend()\n",
    "\n",
    "    # Class distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    bars4 = ax4.bar(class_names, results['per_class_support'], alpha=0.7, color='mediumpurple')\n",
    "    ax4.set_title('Class Distribution (Support)', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Samples')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    for bar, value in zip(bars4, results['per_class_support']):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + max(results['per_class_support']) * 0.01,\n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Per-class metrics saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with normalized values\n",
    "    \n",
    "    Args:\n",
    "        cm: Confusion matrix\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Normalized Frequency'})\n",
    "\n",
    "    plt.title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Add counts\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            plt.text(j + 0.5, i + 0.7, f'({cm[i, j]})',\n",
    "                    ha='center', va='center', fontsize=10, color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confidence_intervals(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confidence intervals for main metrics\n",
    "    \n",
    "    Args:\n",
    "        results: Evaluation results dictionary\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    metrics = ['Accuracy', 'Quadratic Kappa', 'F1 Macro']\n",
    "    values = [results['accuracy'], results['quadratic_kappa'], results['f1_macro']]\n",
    "    ci_lower = [results['accuracy_ci'][0], results['kappa_ci'][0], results['f1_macro_ci'][0]]\n",
    "    ci_upper = [results['accuracy_ci'][1], results['kappa_ci'][1], results['f1_macro_ci'][1]]\n",
    "\n",
    "    errors_lower = [val - lower for val, lower in zip(values, ci_lower)]\n",
    "    errors_upper = [upper - val for val, upper in zip(values, ci_upper)]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "    bars = ax.bar(metrics, values, yerr=[errors_lower, errors_upper],\n",
    "                  capsize=10, alpha=0.7, color=['skyblue', 'lightcoral', 'gold'])\n",
    "\n",
    "    ax.set_title('Main Metrics with 95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    for i, (bar, val, lower, upper) in enumerate(zip(bars, values, ci_lower, ci_upper)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{val:.4f}\\n[{lower:.4f}, {upper:.4f}]',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Confidence intervals saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_attention_maps(model, data_loader, device, num_samples=4, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize attention maps from the model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with attention mechanism\n",
    "        data_loader: DataLoader for samples\n",
    "        device: torch.device\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'get_attention_maps') or not model.use_attention:\n",
    "        print(\"Model doesn't have attention mechanism\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, num_samples * 4))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, img_names in data_loader:\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass to generate attention maps\n",
    "            outputs = model(images)\n",
    "            attention_maps = model.get_attention_maps()\n",
    "\n",
    "            for i in range(min(images.size(0), num_samples - sample_count)):\n",
    "                # Original image\n",
    "                img = images[i].cpu()\n",
    "                img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + \\\n",
    "                      torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "\n",
    "                axes[sample_count, 0].imshow(img.permute(1, 2, 0))\n",
    "                axes[sample_count, 0].set_title(\n",
    "                    f'Original\\nLabel: {labels[i].item() if labels[i] != -1 else \"Unknown\"}',\n",
    "                    fontsize=10\n",
    "                )\n",
    "                axes[sample_count, 0].axis('off')\n",
    "\n",
    "                # Show attention maps\n",
    "                att_idx = 1\n",
    "                for key, att_map in attention_maps.items():\n",
    "                    if att_idx >= 4:\n",
    "                        break\n",
    "                    if 'spatial_attention' in key:\n",
    "                        att = att_map[i, 0].cpu().numpy()\n",
    "                        im = axes[sample_count, att_idx].imshow(att, cmap='jet', alpha=0.7)\n",
    "                        axes[sample_count, att_idx].set_title(\n",
    "                            f'Attention {key.split(\"_\")[-1]}',\n",
    "                            fontsize=10\n",
    "                        )\n",
    "                        axes[sample_count, att_idx].axis('off')\n",
    "                        plt.colorbar(im, ax=axes[sample_count, att_idx], \n",
    "                                   fraction=0.046, pad=0.04)\n",
    "                        att_idx += 1\n",
    "\n",
    "                # Fill remaining slots\n",
    "                while att_idx < 4:\n",
    "                    axes[sample_count, att_idx].axis('off')\n",
    "                    att_idx += 1\n",
    "\n",
    "                sample_count += 1\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "\n",
    "    plt.suptitle('Attention Maps Visualization', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Attention maps saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_fold_comparison(fold_results, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comparison of key metrics across folds\n",
    "    \n",
    "    Args:\n",
    "        fold_results: List of fold result dictionaries\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    if not fold_results:\n",
    "        print(\"No fold results to plot\")\n",
    "        return\n",
    "\n",
    "    fold_numbers = [result['fold'] for result in fold_results]\n",
    "    best_kappas = [result['best_val_kappa'] for result in fold_results]\n",
    "    best_accs = [result['best_val_acc'] for result in fold_results]\n",
    "    best_f1_macros = [result['best_val_f1_macro'] for result in fold_results]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Cross-Validation Performance Summary', fontsize=16, fontweight='bold')\n",
    "\n",
    "    metrics = [\n",
    "        (best_kappas, 'Quadratic Kappa', axes[0]),\n",
    "        (best_accs, 'Accuracy', axes[1]),\n",
    "        (best_f1_macros, 'F1 Macro', axes[2])\n",
    "    ]\n",
    "\n",
    "    for values, label, ax in metrics:\n",
    "        bars = ax.bar(fold_numbers, values, alpha=0.7, color='steelblue')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "        # Add mean line\n",
    "        mean_val = np.mean(values)\n",
    "        ax.axhline(y=mean_val, color='red', linestyle='--', linewidth=2,\n",
    "                  label=f'Mean: {mean_val:.4f}')\n",
    "\n",
    "        # Add std annotation\n",
    "        std_val = np.std(values)\n",
    "        ax.text(0.02, 0.95, f'Std: {std_val:.4f}',\n",
    "               transform=ax.transAxes, ha='left', va='top',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='wheat', alpha=0.8),\n",
    "               fontsize=9)\n",
    "\n",
    "        ax.set_title(f'Best Validation {label}', fontweight='bold')\n",
    "        ax.set_xlabel('Fold')\n",
    "        ax.set_ylabel(label)\n",
    "        ax.set_xticks(fold_numbers)\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Fold comparison saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(results, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for each class\n",
    "    \n",
    "    Args:\n",
    "        results: Evaluation results dictionary\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    all_labels = results['true_labels']\n",
    "    all_probs = results['probabilities']\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        y_true_binary = (all_labels == i).astype(int)\n",
    "        y_prob = all_probs[:, i]\n",
    "\n",
    "        if len(np.unique(y_true_binary)) == 2:\n",
    "            fpr, tpr, _ = roc_curve(y_true_binary, y_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            ax.plot(fpr, tpr, linewidth=2, \n",
    "                   label=f'{class_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curves - One-vs-Rest', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ROC curves saved to {save_path}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ma_detection_env)",
   "language": "python",
   "name": "ma_detection_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
