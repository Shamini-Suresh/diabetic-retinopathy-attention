{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874e930-9330-41f0-aa11-23ed2bc99dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model evaluation utilities with comprehensive metrics\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, cohen_kappa_score, f1_score,\n",
    "    precision_score, recall_score, roc_auc_score,\n",
    "    average_precision_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import contextlib\n",
    "\n",
    "\n",
    "def bootstrap_confidence_interval(y_true, y_pred, metric_func, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate confidence interval for a metric using bootstrap sampling\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        metric_func: Metric function to evaluate\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "        confidence: Confidence level\n",
    "    \n",
    "    Returns:\n",
    "        lower, upper: Confidence interval bounds\n",
    "    \"\"\"\n",
    "    n_samples = len(y_true)\n",
    "    bootstrap_scores = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        y_true_boot = np.array(y_true)[indices]\n",
    "        y_pred_boot = np.array(y_pred)[indices]\n",
    "\n",
    "        try:\n",
    "            score = metric_func(y_true_boot, y_pred_boot)\n",
    "            bootstrap_scores.append(score)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(bootstrap_scores) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_scores, (alpha/2) * 100)\n",
    "    upper = np.percentile(bootstrap_scores, (1 - alpha/2) * 100)\n",
    "\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, class_names=None, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with per-class metrics and confidence intervals\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader for evaluation\n",
    "        device: torch device\n",
    "        class_names: List of class names\n",
    "        confidence_level: Confidence level for intervals\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Skip samples without labels\n",
    "            if (labels == -1).all():\n",
    "                continue\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda') if device.type == 'cuda' else contextlib.nullcontext():\n",
    "                outputs = model(images)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    if len(all_preds) == 0:\n",
    "        print(\"No labeled samples found for evaluation\")\n",
    "        return {}\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate basic metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    quadratic_kappa = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "    linear_kappa = cohen_kappa_score(all_labels, all_preds, weights='linear')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    per_class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "    per_class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "    # Per-class AUC scores\n",
    "    auc_scores = []\n",
    "    per_class_ap = []\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        try:\n",
    "            y_true_binary = (all_labels == i).astype(int)\n",
    "            y_prob = all_probs[:, i]\n",
    "\n",
    "            if len(np.unique(y_true_binary)) == 2:\n",
    "                auc = roc_auc_score(y_true_binary, y_prob)\n",
    "                ap = average_precision_score(y_true_binary, y_prob)\n",
    "                auc_scores.append(auc)\n",
    "                per_class_ap.append(ap)\n",
    "            else:\n",
    "                auc_scores.append(np.nan)\n",
    "                per_class_ap.append(np.nan)\n",
    "        except Exception as e:\n",
    "            auc_scores.append(np.nan)\n",
    "            per_class_ap.append(np.nan)\n",
    "\n",
    "    mean_auc = np.nanmean(auc_scores)\n",
    "    mean_ap = np.nanmean(per_class_ap)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    print(\"Calculating confidence intervals...\")\n",
    "\n",
    "    def accuracy_func(y_true, y_pred):\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "\n",
    "    def kappa_func(y_true, y_pred):\n",
    "        return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "    def f1_macro_func(y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    acc_ci_lower, acc_ci_upper = bootstrap_confidence_interval(\n",
    "        all_labels, all_preds, accuracy_func, confidence=confidence_level)\n",
    "\n",
    "    kappa_ci_lower, kappa_ci_upper = bootstrap_confidence_interval(\n",
    "        all_labels, all_preds, kappa_func, confidence=confidence_level)\n",
    "\n",
    "    f1_ci_lower, f1_ci_upper = bootstrap_confidence_interval(\n",
    "        all_labels, all_preds, f1_macro_func, confidence=confidence_level)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=class_names,\n",
    "                                         zero_division=0, output_dict=True)\n",
    "\n",
    "    # Per-class support\n",
    "    per_class_support = [np.sum(all_labels == i) for i in range(len(class_names))]\n",
    "\n",
    "    results = {\n",
    "        # Basic metrics\n",
    "        'accuracy': accuracy,\n",
    "        'quadratic_kappa': quadratic_kappa,\n",
    "        'linear_kappa': linear_kappa,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'mean_auc': mean_auc,\n",
    "        'mean_ap': mean_ap,\n",
    "\n",
    "        # Per-class metrics\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall,\n",
    "        'per_class_auc': auc_scores,\n",
    "        'per_class_ap': per_class_ap,\n",
    "        'per_class_support': per_class_support,\n",
    "\n",
    "        # Confidence intervals\n",
    "        'accuracy_ci': (acc_ci_lower, acc_ci_upper),\n",
    "        'kappa_ci': (kappa_ci_lower, kappa_ci_upper),\n",
    "        'f1_macro_ci': (f1_ci_lower, f1_ci_upper),\n",
    "\n",
    "        # Other\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': class_report,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION RESULTS WITH CONFIDENCE INTERVALS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} [{acc_ci_lower:.4f}, {acc_ci_upper:.4f}]\")\n",
    "    print(f\"  Quadratic Kappa: {quadratic_kappa:.4f} [{kappa_ci_lower:.4f}, {kappa_ci_upper:.4f}]\")\n",
    "    print(f\"  Linear Kappa: {linear_kappa:.4f}\")\n",
    "    print(f\"  F1 Score (Macro): {f1_macro:.4f} [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "    print(f\"  F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "    print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    print(f\"{'Class':<20} {'Support':<10} {'F1':<10} {'Precision':<12} {'Recall':<10}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        support = per_class_support[i]\n",
    "        f1 = per_class_f1[i] if i < len(per_class_f1) else 0.0\n",
    "        precision = per_class_precision[i] if i < len(per_class_precision) else 0.0\n",
    "        recall = per_class_recall[i] if i < len(per_class_recall) else 0.0\n",
    "\n",
    "        print(f\"{class_name:<20} {support:<10} {f1:<10.4f} {precision:<12.4f} {recall:<10.4f}\")\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ma_detection_env)",
   "language": "python",
   "name": "ma_detection_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
